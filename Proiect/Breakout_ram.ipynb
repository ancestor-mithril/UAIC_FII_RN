{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "from keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import gym\n",
    "import sys\n",
    "from gym import error, spaces\n",
    "from gym import utils\n",
    "from gym.utils import seeding\n",
    "try:\n",
    "    import atari_py\n",
    "except ImportError as e:\n",
    "    raise error.DependencyNotInstalled(\n",
    "            \"{}. (HINT: you can install Atari dependencies by running \"\n",
    "            \"'pip install gym[atari]'.)\".format(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    # keras example for breakout\n",
    "    inputs = layers.Input(shape=(128,), dtype=np.float32)\n",
    "    layer4 = layers.Flatten()(inputs)\n",
    "    layer5 = layers.Dense(512, activation=\"relu\")(layer4)\n",
    "    layer6 = layers.Dense(200, activation=\"relu\")(layer5)\n",
    "    action = layers.Dense(4, activation=\"linear\")(layer6)\n",
    "    model = keras.Model(inputs=inputs, outputs=action)\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.00025, clipnorm=1.0)  # faster than rmsprop\n",
    "    model.compile(optimizer, loss=keras.losses.Huber())  # Huber for stability\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 128)]             0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               66048     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 200)               102600    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 804       \n",
      "=================================================================\n",
      "Total params: 169,452\n",
      "Trainable params: 169,452\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "second_model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Breakout-ram-v0\")\n",
    "print(\"env:\", env)\n",
    "print(\"env.env:\", env.env)\n",
    "predicted_action = 2\n",
    "previous_lives = 5\n",
    "\n",
    "observation = env.reset()\n",
    "env.step(1)\n",
    "for t in range(50):\n",
    "    env.render()\n",
    "    action = env.action_space.sample()\n",
    "    action =  2 if np.random.random(1)[0] > 0.5 else 3\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    if done:\n",
    "        break\n",
    "    if info[\"ale.lives\"] != previous_lives:\n",
    "        env.step(1)\n",
    "        previous_lives = info[\"ale.lives\"]\n",
    "    if reward > 0:\n",
    "        print(\"Not negative reward\", reward, info)\n",
    "    time.sleep(0.1)\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "\n",
    "time.sleep(2)\n",
    "env.close()\n",
    "# print(\"help:\", help(env.env))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable declaration\n",
    "state_history = []\n",
    "action_history = []\n",
    "reward_history = []\n",
    "next_state_history = []\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.00045, clipnorm=1.0)  # faster than rmsprop # TODO: somehow set the learning rate higher at start, and decreasing it over time\n",
    "loss_function = keras.losses.MSE  # used for stability "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "epsilon = 1.0\n",
    "gamma = 0.79\n",
    "previous_lives = 5\n",
    "max_memory_legth = 10000\n",
    "improvement_check = 100\n",
    "iterations = 0\n",
    "games_played = 0\n",
    "update_second_model = 5000\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward: 0.0, games played: 100, iterations made: 23815\n",
      "Reward: 1.0, games played: 200, iterations made: 48635\n",
      "Reward: -1.0, games played: 300, iterations made: 73172\n",
      "Reward: 0.0, games played: 400, iterations made: 97111\n",
      "Reward: 0.0, games played: 500, iterations made: 121649\n",
      "Reward: -1.0, games played: 600, iterations made: 146456\n",
      "Reward: 1.0, games played: 700, iterations made: 171148\n",
      "Reward: -1.0, games played: 800, iterations made: 195940\n",
      "Reward: 2.0, games played: 900, iterations made: 221959\n",
      "Reward: 2.0, games played: 1000, iterations made: 247763\n",
      "Reward: 2.0, games played: 1100, iterations made: 271939\n",
      "Reward: 1.0, games played: 1200, iterations made: 296452\n",
      "Reward: 0.0, games played: 1300, iterations made: 321119\n",
      "Reward: 1.0, games played: 1400, iterations made: 345107\n",
      "Reward: -1.0, games played: 1500, iterations made: 370597\n",
      "Reward: 0.0, games played: 1600, iterations made: 395536\n",
      "Reward: 3.0, games played: 1700, iterations made: 421849\n",
      "Reward: 0.0, games played: 1800, iterations made: 445935\n",
      "Reward: 2.0, games played: 1900, iterations made: 470723\n",
      "Reward: 1.0, games played: 2000, iterations made: 496871\n",
      "Reward: 0.0, games played: 2100, iterations made: 523388\n",
      "Reward: -1.0, games played: 2200, iterations made: 551059\n",
      "Reward: 5.0, games played: 2300, iterations made: 576842\n",
      "Reward: 1.0, games played: 2400, iterations made: 603434\n",
      "Reward: -1.0, games played: 2500, iterations made: 628902\n",
      "Reward: -1.0, games played: 2600, iterations made: 656352\n",
      "Reward: 2.0, games played: 2700, iterations made: 683109\n",
      "Reward: 1.0, games played: 2800, iterations made: 711371\n",
      "Reward: 0.0, games played: 2900, iterations made: 738363\n",
      "Reward: 1.0, games played: 3000, iterations made: 763340\n",
      "Reward: 1.0, games played: 3100, iterations made: 789144\n",
      "Reward: 0.0, games played: 3200, iterations made: 815312\n",
      "Reward: 2.0, games played: 3300, iterations made: 843324\n",
      "Reward: 1.0, games played: 3400, iterations made: 872349\n",
      "Reward: 1.0, games played: 3500, iterations made: 899642\n",
      "Reward: -1.0, games played: 3600, iterations made: 924364\n",
      "Reward: 0.0, games played: 3700, iterations made: 950169\n",
      "Reward: 2.0, games played: 3800, iterations made: 977776\n",
      "Reward: -1.0, games played: 3900, iterations made: 1002880\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-2aae6e13f22e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# agent must predict action, exploatation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[0mcurrent_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_history\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# takes last 4 known states\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m             \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcurrent_state\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# gets reward predictions for both actions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m             \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;36m3\u001b[0m  \u001b[1;31m# choses the actions with the greatest predicted reward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m             \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\Lab_1\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1606\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1607\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1608\u001b[1;33m           steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[0;32m   1609\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1610\u001b[0m       \u001b[1;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\Lab_1\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001b[0m\n\u001b[0;32m   1110\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1111\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mds_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1112\u001b[1;33m         model=model)\n\u001b[0m\u001b[0;32m   1113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1114\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mds_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\Lab_1\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[0;32m    351\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mflat_dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 353\u001b[1;33m     \u001b[0mindices_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflat_map\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mslice_batch_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    354\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    355\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mslice_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\Lab_1\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36mflat_map\u001b[1;34m(self, map_func)\u001b[0m\n\u001b[0;32m   1835\u001b[0m       \u001b[0mDataset\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1836\u001b[0m     \"\"\"\n\u001b[1;32m-> 1837\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mFlatMapDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1838\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1839\u001b[0m   def interleave(self,\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\Lab_1\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, input_dataset, map_func)\u001b[0m\n\u001b[0;32m   4283\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_input_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4284\u001b[0m     self._map_func = StructuredFunctionWrapper(\n\u001b[1;32m-> 4285\u001b[1;33m         map_func, self._transformation_name(), dataset=input_dataset)\n\u001b[0m\u001b[0;32m   4286\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_map_func\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_structure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDatasetSpec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4287\u001b[0m       raise TypeError(\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\Lab_1\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[0;32m   3523\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mtracking\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresource_tracker_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_tracker\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3524\u001b[0m         \u001b[1;31m# TODO(b/141462134): Switch to using garbage collection.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3525\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_concrete_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3526\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0madd_to_graph\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3527\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_to_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\Lab_1\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mget_concrete_function\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3050\u001b[0m     \"\"\"\n\u001b[0;32m   3051\u001b[0m     graph_function = self._get_concrete_function_garbage_collected(\n\u001b[1;32m-> 3052\u001b[1;33m         *args, **kwargs)\n\u001b[0m\u001b[0;32m   3053\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_garbage_collector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3054\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\Lab_1\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3017\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3018\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3019\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3020\u001b[0m       \u001b[0mseen_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3021\u001b[0m       captured = object_identity.ObjectIdentitySet(\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\Lab_1\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3359\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3360\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3361\u001b[1;33m           \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3362\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\Lab_1\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3204\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3205\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3206\u001b[1;33m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[0;32m   3207\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3208\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\Lab_1\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    905\u001b[0m       \u001b[0mkwarg_shapes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    906\u001b[0m     func_args = _get_defun_inputs_from_args(\n\u001b[1;32m--> 907\u001b[1;33m         args, arg_names, flat_shapes=arg_shapes)\n\u001b[0m\u001b[0;32m    908\u001b[0m     func_kwargs = _get_defun_inputs_from_kwargs(\n\u001b[0;32m    909\u001b[0m         kwargs, flat_shapes=kwarg_shapes)\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\Lab_1\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36m_get_defun_inputs_from_args\u001b[1;34m(args, names, flat_shapes)\u001b[0m\n\u001b[0;32m   1141\u001b[0m   \u001b[1;34m\"\"\"Maps Python function positional args to graph-construction inputs.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1142\u001b[0m   return _get_defun_inputs(\n\u001b[1;32m-> 1143\u001b[1;33m       args, names, structure=args, flat_shapes=flat_shapes)\n\u001b[0m\u001b[0;32m   1144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1145\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\Lab_1\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36m_get_defun_inputs\u001b[1;34m(args, names, structure, flat_shapes)\u001b[0m\n\u001b[0;32m   1227\u001b[0m           placeholder.op._set_attr(  # pylint: disable=protected-access\n\u001b[0;32m   1228\u001b[0m               \u001b[1;34m\"_user_specified_name\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1229\u001b[1;33m               attr_value_pb2.AttrValue(s=compat.as_bytes(requested_name)))\n\u001b[0m\u001b[0;32m   1230\u001b[0m         \u001b[0mfunction_inputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1231\u001b[0m       elif isinstance(arg, (resource_variable_ops.BaseResourceVariable,\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\Lab_1\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_set_attr\u001b[1;34m(self, attr_name, attr_value)\u001b[0m\n\u001b[0;32m   2470\u001b[0m         compat.as_bytes(attr_value.SerializeToString()))\n\u001b[0;32m   2471\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2472\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_attr_with_buf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattr_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2473\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2474\u001b[0m       \u001b[0mpywrap_tf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_DeleteBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\Lab_1\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_set_attr_with_buf\u001b[1;34m(self, attr_name, attr_buf)\u001b[0m\n\u001b[0;32m   2478\u001b[0m     \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2479\u001b[0m     pywrap_tf_session.SetAttr(self._graph._c_graph, self._c_op, attr_name,\n\u001b[1;32m-> 2480\u001b[1;33m                               attr_buf)\n\u001b[0m\u001b[0;32m   2481\u001b[0m     \u001b[1;31m# pylint: enable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2482\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Breakout-ram-v4\")  # is better than v0\n",
    "while True:  # training continues forever\n",
    "    observation = env.reset()  # reseting env\n",
    "    state = observation\n",
    "#     env.step(1)  # FIRE triggers ball initialization\n",
    "    episode_reward = 0  # initializes reward for the new episode which follows\n",
    "    while True:  # agent still has lives\n",
    "#         env.render()\n",
    "        iterations += 1  # increasing total iterations of game\n",
    "        if epsilon > np.random.rand(1)[0]:  # a leap of fate, exploration\n",
    "            action = 2 if np.random.random(1)[0] > 0.5 else 3  # random left or right | 3 is left, 2 is right\n",
    "            action = np.random.choice(4)\n",
    "        else:  # agent must predict action, exploatation\n",
    "            current_state = np.array(state_history[-1])  # takes last 4 known states\n",
    "            predictions = model.predict(np.array([current_state]), verbose=0)  # gets reward predictions for both actions\n",
    "            action = 2 if predictions[0, 0] > predictions[0 , 1] else 3  # choses the actions with the greatest predicted reward\n",
    "            action = np.argmax(predictions[0])\n",
    "            \n",
    "        if epsilon > 0.15:  # decay over time is applied to epsilon until it reaches critical value\n",
    "            epsilon -= epsilon / 1000000  # * np.random.random(1)   # decrease is done by (at least) 0.01 %, critical value is reached in (at least) 29956 steps\n",
    "        \n",
    "        observation, reward, done, info = env.step(action)  # action is played, returns new observation, possible reward, done flag and lives remained\n",
    "        next_state = (observation)  \n",
    "        \n",
    "        if info[\"ale.lives\"] != previous_lives:  # if number of lives decreased during this frame\n",
    "#             env.step(1)  # FIRE resummons ball\n",
    "            previous_lives = info[\"ale.lives\"]  # updates previous_lives with current lives\n",
    "#             reward -= 10  # updates reward with negative value because a life was lost\n",
    "            if done:\n",
    "                reward -= 1\n",
    "            \n",
    "        # uncomment this later\n",
    "        # if reward == 0:  # if no reward is received\n",
    "        #     reward -= 0.1  # reward receives small negative value, should encourage the agent to finish the game faster\n",
    "        \n",
    "        # saving values\n",
    "        state_history.append(state)\n",
    "        action_history.append(action)\n",
    "        reward_history.append(reward)\n",
    "        next_state_history.append(next_state)  # next_state of state_history[3] = state_history[4]  # TODO: only use state_history\n",
    "        state = next_state  # replaces old state with new one\n",
    "        \n",
    "        episode_reward += reward  # increases reward for this episode, for checking out improvements for games\n",
    "        \n",
    "        # TODO: apply backprop sometimes in the future\n",
    "        \n",
    "        # Start Back Prop\n",
    "        \n",
    "        if iterations % batch_size == 0:  # doing backprop once every 32 steps\n",
    "            indices = np.random.choice(range(4, len(action_history) - 1), size=batch_size)  # get only indices that have at least 4 previous states, and 1 next state\n",
    "            \n",
    "            state_sample = np.array([state_history[i] for i in indices])  # takes groups of 4 images of game board, previous and except current index\n",
    "            next_state_sample = np.array([state_history[i + 1] for i in indices]) # takes gropus of 4 images of game board, previous and including current index  \n",
    "            reward_sample = np.array([reward_history[i] for i in indices])  # has shape (32,)\n",
    "            action_sample = [action_history[i] for i in indices]  # has len 32; 2 is decreased from each action to transform it into 0 or 1, to minimize one_hot_vectors size\n",
    "            future_rewards = np.amax(second_model.predict(next_state_sample, verbose=0), axis=1)  # gets maximum prediction using second model of future rewards for each next state sample\n",
    "            updated_q_values = reward_sample + gamma * future_rewards  # for current state, adds reward obtained to next state predicted max reward\n",
    "            masks = to_categorical(action_sample, num_classes=4)  # one hot masks are created for actions, to apply backprop only for chosen actions\n",
    "        \n",
    "            with tf.GradientTape() as tape:  # Copied example from keras q-learning. Applies backpropagation to model\n",
    "                q_values = model(state_sample)  # same as `q_values = model.predict(state_sample, verbose=0)`, but returns tensor\n",
    "                q_action = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)  # same as `q_action = np.sum(q_values * masks, axis=1)`, but returns tensor\n",
    "                loss = loss_function(updated_q_values, q_action)  # calculates the loss between updated_q_values, which are correct labels expected, and q_action is the output obtained\n",
    "                grads = tape.gradient(loss, model.trainable_variables)  # yess, applies gradient to weights\n",
    "                optimizer.apply_gradients(zip(grads, model.trainable_variables))  # yess, uses optimizer to update wigths\n",
    "        \n",
    "        \n",
    "        # End Back Prop\n",
    "        \n",
    "        if iterations % update_second_model == 0:  # once every 5000 iterations\n",
    "            second_model.set_weights(model.get_weights())  # updates second model\n",
    "        \n",
    "        if len(action_history) > max_memory_legth:  # if max memory was reached\n",
    "            del state_history[:5000]  # deletes first 5000 elements from each list\n",
    "            del action_history[:5000]\n",
    "            del reward_history[:5000]\n",
    "            del next_state_history[:5000]\n",
    "            \n",
    "        if done:  # end game flag\n",
    "            games_played += 1  # increasing played games\n",
    "            if games_played % improvement_check == 0:  # once every 100 played games\n",
    "                model.save(\"a.h5\")\n",
    "                print(f\"Reward: {episode_reward}, games played: {games_played}, iterations made: {iterations}\")\n",
    "            break  # exits current game\n",
    "\n",
    "    \n",
    "    if iterations % 10000 == 0:\n",
    "        print(f\"Reward: {episode_reward}, games played: {games_played}, iterations made: {iterations}\")\n",
    "        print(games_played)\n",
    "# env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
