{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym[atari] in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (0.18.0)\n",
      "Requirement already satisfied: numpy>=1.10.4 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from gym[atari]) (1.19.4)\n",
      "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from gym[atari]) (1.6.0)\n",
      "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from gym[atari]) (1.5.0)\n",
      "Requirement already satisfied: Pillow<=7.2.0 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from gym[atari]) (7.2.0)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from gym[atari]) (1.5.4)\n",
      "Requirement already satisfied: atari-py~=0.2.0; extra == \"atari\" in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from gym[atari]) (0.2.6)\n",
      "Requirement already satisfied: opencv-python>=3.; extra == \"atari\" in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from gym[atari]) (4.4.0.46)\n",
      "Requirement already satisfied: future in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from pyglet<=1.5.0,>=1.4.0->gym[atari]) (0.18.2)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from atari-py~=0.2.0; extra == \"atari\"->gym[atari]) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install gym[atari]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (1.19.4)\n",
      "Requirement already satisfied: gym in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (0.18.0)\n",
      "Requirement already satisfied: Pillow<=7.2.0 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from gym) (7.2.0)\n",
      "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from gym) (1.5.0)\n",
      "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from gym) (1.6.0)\n",
      "Requirement already satisfied: numpy>=1.10.4 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from gym) (1.19.4)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from gym) (1.5.4)\n",
      "Requirement already satisfied: future in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.18.2)\n",
      "Requirement already satisfied: keras in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (2.4.3)\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from keras) (1.19.4)\n",
      "Requirement already satisfied: h5py in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from keras) (2.10.0)\n",
      "Requirement already satisfied: scipy>=0.14 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from keras) (1.5.4)\n",
      "Requirement already satisfied: pyyaml in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from keras) (5.3.1)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from h5py->keras) (1.15.0)\n",
      "Requirement already satisfied: matplotlib in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (3.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: numpy>=1.15 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from matplotlib) (1.19.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from matplotlib) (7.2.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from cycler>=0.10->matplotlib) (1.15.0)\n",
      "Requirement already satisfied: opencv-python in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (4.4.0.46)\n",
      "Requirement already satisfied: numpy>=1.14.5 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from opencv-python) (1.19.4)\n",
      "Requirement already satisfied: tensorflow-gpu in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (2.4.0)\n",
      "Requirement already satisfied: absl-py~=0.10 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from tensorflow-gpu) (0.11.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from tensorflow-gpu) (3.14.0)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from tensorflow-gpu) (1.12.1)\n",
      "Requirement already satisfied: gast==0.3.3 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from tensorflow-gpu) (0.3.3)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from tensorflow-gpu) (3.7.4.3)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from tensorflow-gpu) (1.12)\n",
      "Requirement already satisfied: tensorboard~=2.4 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from tensorflow-gpu) (2.4.0)\n",
      "Requirement already satisfied: grpcio~=1.32.0 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from tensorflow-gpu) (1.32.0)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from tensorflow-gpu) (1.6.3)\n",
      "Requirement already satisfied: wheel~=0.35 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from tensorflow-gpu) (0.35.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0rc0 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from tensorflow-gpu) (2.4.0)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from tensorflow-gpu) (1.1.2)\n",
      "Requirement already satisfied: google-pasta~=0.2 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from tensorflow-gpu) (0.2.0)\n",
      "Requirement already satisfied: numpy~=1.19.2 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from tensorflow-gpu) (1.19.4)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from tensorflow-gpu) (1.1.0)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from tensorflow-gpu) (3.3.0)\n",
      "Requirement already satisfied: six~=1.15.0 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from tensorflow-gpu) (1.15.0)\n",
      "Requirement already satisfied: h5py~=2.10.0 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from tensorflow-gpu) (2.10.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from tensorboard~=2.4->tensorflow-gpu) (1.24.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from tensorboard~=2.4->tensorflow-gpu) (1.7.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from tensorboard~=2.4->tensorflow-gpu) (3.3.3)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from tensorboard~=2.4->tensorflow-gpu) (1.0.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from tensorboard~=2.4->tensorflow-gpu) (49.6.0.post20200925)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from tensorboard~=2.4->tensorflow-gpu) (0.4.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from tensorboard~=2.4->tensorflow-gpu) (2.25.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow-gpu) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow-gpu) (4.2.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow-gpu) (4.6)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow-gpu) (2.0.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow-gpu) (1.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow-gpu) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow-gpu) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow-gpu) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow-gpu) (1.26.2)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow-gpu) (0.4.8)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow-gpu) (3.4.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow-gpu) (3.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy\n",
    "!pip install  gym\n",
    "!pip install keras\n",
    "!pip install matplotlib\n",
    "!pip install opencv-python\n",
    "!pip install tensorflow-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "from keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import gym\n",
    "import sys\n",
    "from gym import error, spaces\n",
    "from gym import utils\n",
    "from gym.utils import seeding\n",
    "try:\n",
    "    import atari_py\n",
    "except ImportError as e:\n",
    "    raise error.DependencyNotInstalled(\n",
    "            \"{}. (HINT: you can install Atari dependencies by running \"\n",
    "            \"'pip install gym[atari]'.)\".format(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    outputs = 2\n",
    "    # keras example for breakout\n",
    "    inputs = layers.Input(shape=(105, 80, 4), dtype=np.float32)\n",
    "    layer1 = layers.Conv2D(32, 8, strides=4, activation=\"relu\")(inputs)\n",
    "    layer2 = layers.Conv2D(64, 4, strides=2, activation=\"relu\")(layer1)\n",
    "    layer3 = layers.Conv2D(64, 3, strides=1, activation=\"relu\")(layer2)\n",
    "    layer4 = layers.Flatten()(layer3)\n",
    "    layer5 = layers.Dense(512, activation=\"relu\")(layer4)\n",
    "    action = layers.Dense(outputs, activation=\"linear\")(layer5)\n",
    "    model = keras.Model(inputs=inputs, outputs=action)\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.00025, clipnorm=1.0)  # faster than rmsprop\n",
    "    model.compile(optimizer, loss=keras.losses.Huber())  # Huber for stability\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         [(None, 105, 80, 4)]      0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 25, 19, 32)        8224      \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 11, 8, 64)         32832     \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 9, 6, 64)          36928     \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 3456)              0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 512)               1769984   \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 2)                 1026      \n",
      "=================================================================\n",
      "Total params: 1,848,994\n",
      "Trainable params: 1,848,994\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "second_model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb_to_greyscale(observation):\n",
    "    observation = observation[:,:,0] + observation[:,:,1] + observation[:,:,2]\n",
    "    return np.where(observation > 0, 255, 0)[::2, ::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable declaration\n",
    "state_history = []\n",
    "action_history = []\n",
    "reward_history = []\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=0.01,\n",
    "    decay_steps=100000,\n",
    "    decay_rate=0.9\n",
    ")\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.00025, clipnorm=1.0)  # faster than rmsprop\n",
    "loss_function = keras.losses.Huber()  # used for stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "epsilon = 1.0\n",
    "gamma = 0.5  # values tried from 0.5 to 0.99\n",
    "previous_lives = 5\n",
    "max_memory_legth = 10000\n",
    "improvement_check = 100\n",
    "iterations = 0\n",
    "games_played = 0\n",
    "update_second_model = 5000\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"BreakoutNoFrameskip-v4\")  # tried v0 and deterministic\n",
    "while True:  # training continues forever\n",
    "    observation = env.reset()  # reseting env\n",
    "    state = rgb_to_greyscale(observation)  # converts observation (210, 160, 3), to greydownscaled state, (105, 80)\n",
    "    \n",
    "    # when predictiong only LEFT or RIGHT, we have to manually FIRE\n",
    "    env.step(1)  # FIRE triggers ball initialization\n",
    "    \n",
    "    \n",
    "    episode_reward = 0  # initializes reward for the new episode which follows\n",
    "    \n",
    "    while True:  # agent still has lives\n",
    "#         env.render()\n",
    "\n",
    "        iterations += 1  # increasing total iterations of game\n",
    "        if epsilon > np.random.rand(1)[0]:  # a leap of fate, exploration\n",
    "            # case of model output being only LEFT / RIGHT \n",
    "            action = 2 if np.random.random(1)[0] > 0.5 else 3  # random left or right | 3 is left, 2 is right\n",
    "            \n",
    "            # case of model output being all 4 actions: NOOP, FIRE, LEFT, RIGHT\n",
    "#             action = np.random.choice(4)\n",
    "        else:  # agent must predict action, exploatation\n",
    "            four_states = np.array(state_history[-4:])  # takes last 4 known states\n",
    "            four_states = four_states.reshape(105, 80, 4)  # reshapes them into input shape \n",
    "            predictions = model.predict(np.array([four_states]), verbose=0)  # gets reward predictions for both actions\n",
    "            # case of model output being only LEFT / RIGHT \n",
    "            action = 2 if predictions[0, 0] > predictions[0 , 1] else 3  # choses the actions with the greatest predicted reward\n",
    "            \n",
    "            # case of model output being all 4 actions: NOOP, FIRE, LEFT, RIGHT\n",
    "#             action = np.argmax(predictions[0])\n",
    "            \n",
    "        if epsilon > 0.05:  # decay over time is applied to epsilon until it reaches critical value\n",
    "            epsilon -= epsilon / 1000000  # critical value is reached in 2995732 steps\n",
    "    \n",
    "        # we should be more interested about total reward later in training, at the beggining we should be greedy for reward\n",
    "        if gamma < 0.99:\n",
    "            gamma += gamma / 1000000  \n",
    "        \n",
    "        observation, reward, done, info = env.step(action)  # action is played, returns new observation, possible reward, done flag and lives remained\n",
    "        next_state = rgb_to_greyscale(observation)  # converts observation (210, 160, 3), to greydownscaled state, (105, 80) \n",
    "        \n",
    "        if info[\"ale.lives\"] != previous_lives:  # if number of lives decreased during this frame\n",
    "            env.step(1)  # FIRE resummons ball\n",
    "            previous_lives = info[\"ale.lives\"]  # updates previous_lives with current lives\n",
    "            # here we tried different negative rewards, including no negative reward for life lost and including no negative reward for DONE\n",
    "#             reward -= 1  # updates reward with negative value because a life was lost\n",
    "            if done:  # if game is finished, agent lost\n",
    "                reward -= 1\n",
    "            \n",
    "        # we also tried this, the purpose being to encourage the agent to try to make the ball reach above bricks and finish the game faster, but it never reached that state\n",
    "        # uncomment this later\n",
    "        # if reward == 0:  # if no reward is received\n",
    "        #     reward -= 0.1  # reward receives small negative value, should encourage the agent to finish the game faster\n",
    "        \n",
    "        # saving values\n",
    "        state_history.append(state)\n",
    "        action_history.append(action)\n",
    "        reward_history.append(reward)\n",
    "        state = next_state  # replaces old state with new one\n",
    "        \n",
    "        episode_reward += reward  # increases reward for this episode, for checking out improvements for games\n",
    "        \n",
    "        # Start Back Prop\n",
    "        \n",
    "        if iterations % 4 == 0 and len(action_history) > batch_size:  # doing backprop once every 4, 16 or 32 steps \n",
    "            \n",
    "            indices = np.random.choice(range(4, len(action_history) - 1), size=batch_size)  # get only indices that have at least 4 previous states, and 1 next state\n",
    "            \n",
    "            state_sample = np.array([state_history[i-4:i] for i in indices])  # takes groups of 4 images of game board, previous and except current index\n",
    "            state_sample = state_sample.reshape(batch_size, 105, 80, 4)  # reshapes group from (32, 4, 105, 80) to (32, 105, 80, 4)\n",
    "            next_state_sample = np.array([state_history[i - 3: i + 1] for i in indices]) # takes gropus of 4 images of game board, previous and including current index\n",
    "            next_state_sample = next_state_sample.reshape(batch_size, 105, 80, 4)  \n",
    "            reward_sample = np.array([reward_history[i] for i in indices])  # has shape (32,)\n",
    "            action_sample = [action_history[i] - 2 for i in indices]  \n",
    "            # has len 32; 2 is decreased from each action to transform it into 0 or 1, to minimize one_hot_vectors size\n",
    "            # when using all 4 actions, -2 is not decreased\n",
    "            future_rewards = np.amax(second_model.predict(next_state_sample, verbose=0), axis=1)  \n",
    "            # gets maximum prediction using second model of future rewards for each next state sample\n",
    "            updated_q_values = reward_sample + gamma * future_rewards  # for current state, adds reward obtained to next state predicted max reward\n",
    "            masks = to_categorical(action_sample, num_classes=2)  # one hot masks are created for actions, to apply backprop only for chosen actions\n",
    "            \n",
    "            # The code for when we tried to do backprop by using model.fit\n",
    "#             q_values = model.predict(state_sample, verbose=0)  # here we predict the values\n",
    "#             q_action = np.sum(q_values * masks, axis=1)  # here we get the reward predicted for chosen action\n",
    "            \n",
    "#             true_labels = second_model.predict(next_state_sample, verbose=0)  # here we get again the predictions\n",
    "#             j = 0\n",
    "#             for i in true_labels:\n",
    "#                 true_labels[np.argmax(i)] = updated_q_values[j]  # here we modify predictions for chosen actions with updated_q_values = reward  + gamma*  next_state prediction \n",
    "#                 j += 1\n",
    "#             state_sample = tf.convert_to_tensor(state_sample)  \n",
    "#             true_labels = tf.convert_to_tensor(true_labels)  \n",
    "#             model.fit(\n",
    "#                 state_sample,\n",
    "#                 true_labels,\n",
    "#                 epochs=1,\n",
    "#                 verbose=0,\n",
    "#                 batch_size=32\n",
    "#             )\n",
    "\n",
    "            # faster (and better) than model.fit\n",
    "            with tf.GradientTape() as tape:  # Copied example from keras q-learning. Applies backpropagation to model\n",
    "                # here we needed to use tensors, otherwise error\n",
    "                q_values = model(state_sample)  # same as `q_values = model.predict(state_sample, verbose=0)`, but returns tensor\n",
    "                q_action = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)  # same as `q_action = np.sum(q_values * masks, axis=1)`, but returns tensor\n",
    "                loss = loss_function(updated_q_values, q_action)  \n",
    "                # calculates the loss between updated_q_values, which are correct labels expected, and q_action is the output obtained\n",
    "                grads = tape.gradient(loss, model.trainable_variables)  # applies gradient to weights\n",
    "                optimizer.apply_gradients(zip(grads, model.trainable_variables))  # uses optimizer to update wigths\n",
    "            \n",
    "        \n",
    "        # End Back Prop\n",
    "        \n",
    "        if iterations % update_second_model == 0:  # once every 5000/other numbers iterations\n",
    "            second_model.set_weights(model.get_weights())  # updates second model\n",
    "        \n",
    "        if len(action_history) > max_memory_legth:  # if max memory was reached\n",
    "            del state_history[:5000]  # deletes first 5000 elements from each list, we also tried to delete only 1\n",
    "            del action_history[:5000]\n",
    "            del reward_history[:5000]\n",
    "            del next_state_history[:5000]\n",
    "            \n",
    "        if done:  # end game flag\n",
    "            games_played += 1  # increasing played games\n",
    "            if games_played % 10 == 0:  # once every 100/other constant played games\n",
    "                model.save(\"a1.h5\")\n",
    "                print(f\"Reward: {episode_reward}, games played: {games_played}, iterations made: {iterations}\")\n",
    "            break  # exits current game\n",
    "\n",
    "    \n",
    "    if iterations % 10000 == 0:\n",
    "        print(f\"Reward: {episode_reward}, games played: {games_played}, iterations made: {iterations}\")\n",
    "        print(games_played)\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.096"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
