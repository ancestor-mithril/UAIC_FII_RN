{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym[atari] in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (0.18.0)\n",
      "Requirement already satisfied: numpy>=1.10.4 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from gym[atari]) (1.19.4)\n",
      "Requirement already satisfied: Pillow<=7.2.0 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from gym[atari]) (7.2.0)\n",
      "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from gym[atari]) (1.6.0)\n",
      "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from gym[atari]) (1.5.0)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from gym[atari]) (1.5.4)\n",
      "Requirement already satisfied: atari-py~=0.2.0; extra == \"atari\" in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from gym[atari]) (0.2.6)\n",
      "Requirement already satisfied: opencv-python>=3.; extra == \"atari\" in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from gym[atari]) (4.4.0.46)\n",
      "Requirement already satisfied: future in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from pyglet<=1.5.0,>=1.4.0->gym[atari]) (0.18.2)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from atari-py~=0.2.0; extra == \"atari\"->gym[atari]) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install gym[atari]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (1.19.4)\n",
      "Requirement already satisfied: gym in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (0.18.0)\n",
      "Requirement already satisfied: Pillow<=7.2.0 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from gym) (7.2.0)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from gym) (1.5.4)\n",
      "Requirement already satisfied: numpy>=1.10.4 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from gym) (1.19.4)\n",
      "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from gym) (1.6.0)\n",
      "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from gym) (1.5.0)\n",
      "Requirement already satisfied: future in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.18.2)\n",
      "Requirement already satisfied: keras in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (2.4.3)\n",
      "Requirement already satisfied: h5py in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from keras) (2.10.0)\n",
      "Requirement already satisfied: scipy>=0.14 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from keras) (1.5.4)\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from keras) (1.19.4)\n",
      "Requirement already satisfied: pyyaml in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from keras) (5.3.1)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from h5py->keras) (1.15.0)\n",
      "Requirement already satisfied: matplotlib in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (3.3.3)\n",
      "Requirement already satisfied: numpy>=1.15 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from matplotlib) (1.19.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from matplotlib) (7.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from cycler>=0.10->matplotlib) (1.15.0)\n",
      "Requirement already satisfied: opencv-python in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (4.4.0.46)\n",
      "Requirement already satisfied: numpy>=1.14.5 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from opencv-python) (1.19.4)\n",
      "Requirement already satisfied: tensorflow-gpu in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (2.4.0)\n",
      "Requirement already satisfied: tensorboard~=2.4 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from tensorflow-gpu) (2.4.0)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from tensorflow-gpu) (1.1.0)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from tensorflow-gpu) (3.7.4.3)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from tensorflow-gpu) (1.1.2)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from tensorflow-gpu) (1.12.1)\n",
      "Requirement already satisfied: gast==0.3.3 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from tensorflow-gpu) (0.3.3)\n",
      "Requirement already satisfied: numpy~=1.19.2 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from tensorflow-gpu) (1.19.4)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from tensorflow-gpu) (3.14.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0rc0 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from tensorflow-gpu) (2.4.0)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from tensorflow-gpu) (3.3.0)\n",
      "Requirement already satisfied: six~=1.15.0 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from tensorflow-gpu) (1.15.0)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from tensorflow-gpu) (1.12)\n",
      "Requirement already satisfied: wheel~=0.35 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from tensorflow-gpu) (0.35.1)\n",
      "Requirement already satisfied: h5py~=2.10.0 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from tensorflow-gpu) (2.10.0)\n",
      "Requirement already satisfied: grpcio~=1.32.0 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from tensorflow-gpu) (1.32.0)\n",
      "Requirement already satisfied: absl-py~=0.10 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from tensorflow-gpu) (0.11.0)\n",
      "Requirement already satisfied: google-pasta~=0.2 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from tensorflow-gpu) (0.2.0)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from tensorflow-gpu) (1.6.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from tensorboard~=2.4->tensorflow-gpu) (2.25.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from tensorboard~=2.4->tensorflow-gpu) (49.6.0.post20200925)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from tensorboard~=2.4->tensorflow-gpu) (1.7.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from tensorboard~=2.4->tensorflow-gpu) (0.4.2)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from tensorboard~=2.4->tensorflow-gpu) (1.24.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from tensorboard~=2.4->tensorflow-gpu) (3.3.3)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from tensorboard~=2.4->tensorflow-gpu) (1.0.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow-gpu) (1.26.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow-gpu) (2020.12.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow-gpu) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow-gpu) (2.10)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow-gpu) (1.3.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow-gpu) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow-gpu) (4.2.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow-gpu) (4.6)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow-gpu) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow-gpu) (3.1.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow-gpu) (0.4.8)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow-gpu) (3.4.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy\n",
    "!pip install  gym\n",
    "!pip install keras\n",
    "!pip install matplotlib\n",
    "!pip install opencv-python\n",
    "!pip install tensorflow-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "from keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import gym\n",
    "import sys\n",
    "from gym import error, spaces\n",
    "from gym import utils\n",
    "from gym.utils import seeding\n",
    "try:\n",
    "    import atari_py\n",
    "except ImportError as e:\n",
    "    raise error.DependencyNotInstalled(\n",
    "            \"{}. (HINT: you can install Atari dependencies by running \"\n",
    "            \"'pip install gym[atari]'.)\".format(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    # keras example for breakout\n",
    "    inputs = layers.Input(shape=(105, 80, 4), dtype=np.float32)\n",
    "    layer1 = layers.Conv2D(32, 8, strides=4, activation=\"relu\")(inputs)\n",
    "    layer2 = layers.Conv2D(64, 4, strides=2, activation=\"relu\")(layer1)\n",
    "    layer3 = layers.Conv2D(64, 3, strides=1, activation=\"relu\")(layer2)\n",
    "    layer4 = layers.Flatten()(layer3)\n",
    "    layer5 = layers.Dense(512, activation=\"relu\")(layer4)\n",
    "    action = layers.Dense(2, activation=\"linear\")(layer5)\n",
    "    model = keras.Model(inputs=inputs, outputs=action)\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.00025, clipnorm=1.0)  # faster than rmsprop\n",
    "    model.compile(optimizer, loss=keras.losses.Huber())  # Huber for stability\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 105, 80, 4)]      0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 25, 19, 32)        8224      \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 11, 8, 64)         32832     \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 9, 6, 64)          36928     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 3456)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               1769984   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 1026      \n",
      "=================================================================\n",
      "Total params: 1,848,994\n",
      "Trainable params: 1,848,994\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "second_model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb_to_greyscale(observation):\n",
    "    observation = observation[:,:,0] + observation[:,:,1] + observation[:,:,2]\n",
    "    return np.where(observation > 0, 255, 0)[::2, ::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: <TimeLimit<AtariEnv<BreakoutDeterministic-v4>>>\n",
      "env.env: <AtariEnv<BreakoutDeterministic-v4>>\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"BreakoutDeterministic-v4\")\n",
    "print(\"env:\", env)\n",
    "print(\"env.env:\", env.env)\n",
    "predicted_action = 2\n",
    "previous_lives = 5\n",
    "\n",
    "observation = env.reset()\n",
    "env.step(1)\n",
    "for t in range(50):\n",
    "    env.render()\n",
    "    action = env.action_space.sample()\n",
    "    action =  2 if np.random.random(1)[0] > 0.5 else 3\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    if done:\n",
    "        break\n",
    "    if info[\"ale.lives\"] != previous_lives:\n",
    "        env.step(1)\n",
    "        previous_lives = info[\"ale.lives\"]\n",
    "    if reward > 0:\n",
    "        print(\"Not negative reward\", reward, info)\n",
    "    time.sleep(0.1)\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "\n",
    "time.sleep(2)\n",
    "env.close()\n",
    "# print(\"help:\", help(env.env))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(105, 80)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = rgb_to_greyscale(observation)\n",
    "state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(105, 80, 4)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = rgb_to_greyscale(observation)\n",
    "tensor_state = np.array([state, state, state, state])\n",
    "tensor_state = tensor_state.reshape(105, 80, 4)\n",
    "tensor_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 84ms/step\n"
     ]
    }
   ],
   "source": [
    "x = model.predict(np.array([tensor_state]), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.771219"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable declaration\n",
    "state_history = []\n",
    "action_history = []\n",
    "reward_history = []\n",
    "next_state_history = []\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.00025, clipnorm=1.0)  # faster than rmsprop # TODO: somehow set the learning rate higher at start, and decreasing it over time\n",
    "loss_function = keras.losses.Huber()  # used for stability "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 0 into shape (105,80,4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-0a61492e0042>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_history\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mfour_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_history\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# takes last 4 known states\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mfour_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfour_states\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m105\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m80\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# reshapes them into input shape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mfour_states\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfour_states\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 0 into shape (105,80,4)"
     ]
    }
   ],
   "source": [
    "(np.array(state_history)).shape\n",
    "four_states = np.array(state_history[-4:])  # takes last 4 known states\n",
    "four_states = four_states.reshape(105, 80, 4)  # reshapes them into input shape \n",
    "four_states.shape\n",
    "predictions = model.predict(np.array([four_states]), verbose=0)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = 2 if predictions[0, 0] > predictions[0 , 1] else 3\n",
    "action\n",
    "env.render()\n",
    "_, _, _, _ = env.step(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "epsilon = 1.0\n",
    "gamma = 0.5\n",
    "previous_lives = 5\n",
    "max_memory_legth = 10000\n",
    "improvement_check = 100\n",
    "iterations = 0\n",
    "games_played = 0\n",
    "update_second_model = 5000\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward: -55.0, games played: 100, iterations made: 17096\n",
      "Reward: -60.0, games played: 200, iterations made: 33059\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[55,475,256] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu [Op:Conv2DBackpropInput]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-69523de17cce>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     61\u001b[0m                 \u001b[0mq_action\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# same as `q_action = np.sum(q_values * masks, axis=1)`, but returns tensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mupdated_q_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mq_action\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# calculates the loss between updated_q_values, which are correct labels expected, and q_action is the output obtained\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# yess, applies gradient to weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# yess, uses optimizer to update wigths\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\Lab_1\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[1;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[0;32m   1084\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1085\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1086\u001b[1;33m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[0;32m   1087\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1088\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\Lab_1\\lib\\site-packages\\tensorflow\\python\\eager\\imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[1;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[0;32m     75\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\Lab_1\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[1;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[0;32m    160\u001b[0m       \u001b[0mgradient_name_scope\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"/\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradient_name_scope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 162\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    163\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\Lab_1\\lib\\site-packages\\tensorflow\\python\\ops\\nn_grad.py\u001b[0m in \u001b[0;36m_Conv2DGrad\u001b[1;34m(op, grad)\u001b[0m\n\u001b[0;32m    594\u001b[0m           \u001b[0mexplicit_paddings\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexplicit_paddings\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    595\u001b[0m           \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 596\u001b[1;33m           data_format=data_format),\n\u001b[0m\u001b[0;32m    597\u001b[0m       gen_nn_ops.conv2d_backprop_filter(\n\u001b[0;32m    598\u001b[0m           \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\Lab_1\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d_backprop_input\u001b[1;34m(input_sizes, filter, out_backprop, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[0;32m   1244\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1245\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1246\u001b[1;33m       \u001b[0m_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1247\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1248\u001b[0m       \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\Lab_1\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   6860\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\" name: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6861\u001b[0m   \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6862\u001b[1;33m   \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6863\u001b[0m   \u001b[1;31m# pylint: enable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6864\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\Lab_1\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[55,475,256] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu [Op:Conv2DBackpropInput]"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"BreakoutDeterministic-v4\")  # is better than v0\n",
    "while True:  # training continues forever\n",
    "    observation = env.reset()  # reseting env\n",
    "    state = rgb_to_greyscale(observation)  # converts observation (210, 160, 3), to greydownscaled state, (105, 80)\n",
    "    env.step(1)  # FIRE triggers ball initialization\n",
    "    episode_reward = 0  # initializes reward for the new episode which follows\n",
    "    while True:  # agent still has lives\n",
    "#         env.render()\n",
    "        iterations += 1  # increasing total iterations of game\n",
    "        if epsilon > np.random.rand(1)[0]:  # a leap of fate, exploration\n",
    "            action = 2 if np.random.random(1)[0] > 0.5 else 3  # random left or right | 3 is left, 2 is right\n",
    "        else:  # agent must predict action, exploatation\n",
    "            four_states = np.array(state_history[-4:])  # takes last 4 known states\n",
    "            four_states = four_states.reshape(105, 80, 4)  # reshapes them into input shape \n",
    "            predictions = model.predict(np.array([four_states]), verbose=0)  # gets reward predictions for both actions\n",
    "            action = 2 if predictions[0, 0] > predictions[0 , 1] else 3  # choses the actions with the greatest predicted reward\n",
    "            \n",
    "        if epsilon > 0.05:  # decay over time is applied to epsilon until it reaches critical value\n",
    "            epsilon -= epsilon / 10000  # * np.random.random(1)   # decrease is done by (at least) 0.01 %, critical value is reached in (at least) 29956 steps\n",
    "        \n",
    "        observation, reward, done, info = env.step(action)  # action is played, returns new observation, possible reward, done flag and lives remained\n",
    "        next_state = rgb_to_greyscale(observation)  # converts observation (210, 160, 3), to greydownscaled state, (105, 80)  # TODO: check if next_state is really needed, we might only use state\n",
    "        \n",
    "        if info[\"ale.lives\"] != previous_lives:  # if number of lives decreased during this frame\n",
    "            env.step(1)  # FIRE resummons ball\n",
    "            previous_lives = info[\"ale.lives\"]  # updates previous_lives with current lives\n",
    "            reward -= 10  # updates reward with negative value because a life was lost\n",
    "            \n",
    "        # uncomment this later\n",
    "        # if reward == 0:  # if no reward is received\n",
    "        #     reward -= 0.1  # reward receives small negative value, should encourage the agent to finish the game faster\n",
    "        \n",
    "        # saving values\n",
    "        state_history.append(state)\n",
    "        action_history.append(action)\n",
    "        reward_history.append(reward)\n",
    "        next_state_history.append(next_state)  # next_state of state_history[3] = state_history[4]  # TODO: only use state_history\n",
    "        state = next_state  # replaces old state with new one\n",
    "        \n",
    "        episode_reward += reward  # increases reward for this episode, for checking out improvements for games\n",
    "        \n",
    "        # TODO: apply backprop sometimes in the future\n",
    "        \n",
    "        # Start Back Prop\n",
    "        \n",
    "        if iterations % batch_size == 0:  # doing backprop once every 32 steps\n",
    "            indices = np.random.choice(range(4, len(action_history)), size=batch_size)  # get only indices that have at least 4 previous states, and 1 next state\n",
    "            \n",
    "            state_sample = np.array([state_history[i-4:i] for i in indices])  # takes groups of 4 images of game board, previous and except current index\n",
    "            state_sample = state_sample.reshape(batch_size, 105, 80, 4)  # reshapes group from (32, 4, 105, 80) to (32, 105, 80, 4)\n",
    "            next_state_sample = np.array([state_history[i - 3: i + 1] for i in indices]) # takes gropus of 4 images of game board, previous and including current index\n",
    "            next_state_sample = next_state_sample.reshape(batch_size, 105, 80, 4)  \n",
    "            reward_sample = np.array([reward_history[i] for i in indices])  # has shape (32,)\n",
    "            action_sample = [action_history[i] - 2 for i in indices]  # has len 32; 2 is decreased from each action to transform it into 0 or 1, to minimize one_hot_vectors size\n",
    "            future_rewards = np.amax(second_model.predict(next_state_sample, verbose=0), axis=1)  # gets maximum prediction using second model of future rewards for each next state sample\n",
    "            updated_q_values = reward_sample + gamma * future_rewards  # for current state, adds reward obtained to next state predicted max reward\n",
    "            masks = to_categorical(action_sample)  # one hot masks are created for actions, to apply backprop only for chosen actions\n",
    "        \n",
    "            with tf.GradientTape() as tape:  # Copied example from keras q-learning. Applies backpropagation to model\n",
    "                q_values = model(state_sample)  # same as `q_values = model.predict(state_sample, verbose=0)`, but returns tensor\n",
    "                q_action = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)  # same as `q_action = np.sum(q_values * masks, axis=1)`, but returns tensor\n",
    "                loss = loss_function(updated_q_values, q_action)  # calculates the loss between updated_q_values, which are correct labels expected, and q_action is the output obtained\n",
    "                grads = tape.gradient(loss, model.trainable_variables)  # yess, applies gradient to weights\n",
    "                optimizer.apply_gradients(zip(grads, model.trainable_variables))  # yess, uses optimizer to update wigths\n",
    "        \n",
    "        \n",
    "        # End Back Prop\n",
    "        \n",
    "        if iterations % update_second_model == 0:  # once every 5000 iterations\n",
    "            second_model.set_weights(model.get_weights())  # updates second model\n",
    "        \n",
    "        if len(action_history) > max_memory_legth:  # if max memory was reached\n",
    "            del state_history[:5000]  # deletes first 5000 elements from each list\n",
    "            del action_history[:5000]\n",
    "            del reward_history[:5000]\n",
    "            del next_state_history[:5000]\n",
    "            \n",
    "        if done:  # end game flag\n",
    "            games_played += 1  # increasing played games\n",
    "            if games_played % improvement_check == 0:  # once every 100 played games\n",
    "                model.save(\"a.h5\")\n",
    "                print(f\"Reward: {episode_reward}, games played: {games_played}, iterations made: {iterations}\")\n",
    "            break  # exits current game\n",
    "\n",
    "    \n",
    "    if iterations % 10000 == 0:\n",
    "        print(f\"Reward: {episode_reward}, games played: {games_played}, iterations made: {iterations}\")\n",
    "        print(games_played)\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('a.h5')\n",
    "second_model = load_model('a.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.step(3)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 748, 1965, 1883,  469,  172, 1240, 1997, 2129, 2483,  845, 2754,\n",
       "       1201,  418, 2433, 1885,  528, 2008,  285, 2663,   13, 2320, 2930,\n",
       "        389,  570, 2221,  222, 2115, 2104, 1277,   56, 2965,  569])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = np.random.choice(range(4, len(action_history) - 1), size=batch_size)\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 105, 80, 4)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_sample = np.array([state_history[i-4:i] for i in indices])\n",
    "state_sample = state_sample.reshape(batch_size, 105, 80, 4)\n",
    "state_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 105, 80, 4)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_state_sample = np.array([state_history[i - 3: i + 1] for i in indices])\n",
    "next_state_sample = next_state_sample.reshape(batch_size, 105, 80, 4)\n",
    "next_state_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32,)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward_sample = np.array([reward_history[i] for i in indices])\n",
    "reward_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_sample = [action_history[i] - 2 for i in indices]\n",
    "len(action_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "future_rewards = np.amax(model.predict(next_state_sample, verbose=0), axis=1)\n",
    "future_rewards.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  7.75147858, -91.85701561,   8.11975327,   7.20004692,\n",
       "         5.73523035,   7.6509284 ,   8.96559372,   7.31376162,\n",
       "         7.96670856,   8.30192795,   8.49255314,   8.34010067,\n",
       "         7.88028183,   8.53532734,   9.04493752,   7.46779385,\n",
       "         8.08306541,   4.59949722,   7.2463974 ,   7.91787567,\n",
       "         8.04298439,   7.06307402,   8.04298439,   6.02660122,\n",
       "         7.16639652,   7.55825224,   9.52005901,   6.95206108,\n",
       "         8.9129776 ,   7.03784075, -91.51414108,   7.07100906])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_q_values = reward_sample + gamma * future_rewards\n",
    "updated_q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 2)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masks = to_categorical(action_sample)\n",
    "masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[14.878384 , -3.0652595],\n",
       "       [16.285969 , -3.1187997],\n",
       "       [17.679295 , -3.5014887],\n",
       "       [15.0685425, -1.8668299],\n",
       "       [15.468747 , -7.6869564],\n",
       "       [17.142412 , -3.7448063],\n",
       "       [16.702974 , -2.5793161],\n",
       "       [15.241731 , -6.9109945],\n",
       "       [17.850702 , -3.1788397],\n",
       "       [16.880201 , -3.655157 ],\n",
       "       [16.380934 , -4.2904115],\n",
       "       [15.942877 , -3.1279793],\n",
       "       [17.294239 , -2.8690991],\n",
       "       [17.466892 , -3.4695053],\n",
       "       [17.09319  , -2.7820396],\n",
       "       [14.201021 , -7.633436 ],\n",
       "       [16.306837 , -3.8131094],\n",
       "       [ 5.4835024, -6.65471  ],\n",
       "       [14.772081 , -3.7185078],\n",
       "       [15.703913 , -2.1918273],\n",
       "       [16.378193 , -4.135713 ],\n",
       "       [15.842686 , -3.5639486],\n",
       "       [16.285969 , -3.1187997],\n",
       "       [14.342003 , -6.874502 ],\n",
       "       [15.771332 , -3.3852835],\n",
       "       [17.586872 , -5.674301 ],\n",
       "       [15.065003 , -6.34674  ],\n",
       "       [14.243395 , -3.1381192],\n",
       "       [17.622715 ,  1.1727142],\n",
       "       [16.08644  , -5.409835 ],\n",
       "       [16.971718 , -2.9901457],\n",
       "       [13.520449 , -5.761632 ]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_values = model.predict(state_sample, verbose=0)\n",
    "q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32,)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_action = np.sum(q_values * masks, axis=1)\n",
    "q_action.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=88.40752>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = model.loss(updated_q_values, q_action)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    q_values = model(state_sample)\n",
    "    q_action = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)\n",
    "    loss = loss_function(updated_q_values, q_action)\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.054547507"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.amax(model.trainable_variables[0].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.054547507"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.amax(model.trainable_variables[0].numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
