{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym[atari] in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (0.18.0)\n",
      "Requirement already satisfied: numpy>=1.10.4 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from gym[atari]) (1.19.4)\n",
      "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from gym[atari]) (1.6.0)\n",
      "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from gym[atari]) (1.5.0)\n",
      "Requirement already satisfied: Pillow<=7.2.0 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from gym[atari]) (7.2.0)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from gym[atari]) (1.5.4)\n",
      "Requirement already satisfied: atari-py~=0.2.0; extra == \"atari\" in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from gym[atari]) (0.2.6)\n",
      "Requirement already satisfied: opencv-python>=3.; extra == \"atari\" in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from gym[atari]) (4.4.0.46)\n",
      "Requirement already satisfied: future in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from pyglet<=1.5.0,>=1.4.0->gym[atari]) (0.18.2)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from atari-py~=0.2.0; extra == \"atari\"->gym[atari]) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install gym[atari]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (1.19.4)\n",
      "Requirement already satisfied: gym in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (0.18.0)\n",
      "Requirement already satisfied: Pillow<=7.2.0 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from gym) (7.2.0)\n",
      "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from gym) (1.5.0)\n",
      "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from gym) (1.6.0)\n",
      "Requirement already satisfied: numpy>=1.10.4 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from gym) (1.19.4)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from gym) (1.5.4)\n",
      "Requirement already satisfied: future in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.18.2)\n",
      "Requirement already satisfied: keras in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (2.4.3)\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from keras) (1.19.4)\n",
      "Requirement already satisfied: h5py in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from keras) (2.10.0)\n",
      "Requirement already satisfied: scipy>=0.14 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from keras) (1.5.4)\n",
      "Requirement already satisfied: pyyaml in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from keras) (5.3.1)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from h5py->keras) (1.15.0)\n",
      "Requirement already satisfied: matplotlib in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (3.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: numpy>=1.15 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from matplotlib) (1.19.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from matplotlib) (7.2.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from cycler>=0.10->matplotlib) (1.15.0)\n",
      "Requirement already satisfied: opencv-python in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (4.4.0.46)\n",
      "Requirement already satisfied: numpy>=1.14.5 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from opencv-python) (1.19.4)\n",
      "Requirement already satisfied: tensorflow-gpu in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (2.4.0)\n",
      "Requirement already satisfied: absl-py~=0.10 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from tensorflow-gpu) (0.11.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from tensorflow-gpu) (3.14.0)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from tensorflow-gpu) (1.12.1)\n",
      "Requirement already satisfied: gast==0.3.3 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from tensorflow-gpu) (0.3.3)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from tensorflow-gpu) (3.7.4.3)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from tensorflow-gpu) (1.12)\n",
      "Requirement already satisfied: tensorboard~=2.4 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from tensorflow-gpu) (2.4.0)\n",
      "Requirement already satisfied: grpcio~=1.32.0 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from tensorflow-gpu) (1.32.0)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from tensorflow-gpu) (1.6.3)\n",
      "Requirement already satisfied: wheel~=0.35 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from tensorflow-gpu) (0.35.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0rc0 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from tensorflow-gpu) (2.4.0)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from tensorflow-gpu) (1.1.2)\n",
      "Requirement already satisfied: google-pasta~=0.2 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from tensorflow-gpu) (0.2.0)\n",
      "Requirement already satisfied: numpy~=1.19.2 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from tensorflow-gpu) (1.19.4)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from tensorflow-gpu) (1.1.0)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from tensorflow-gpu) (3.3.0)\n",
      "Requirement already satisfied: six~=1.15.0 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from tensorflow-gpu) (1.15.0)\n",
      "Requirement already satisfied: h5py~=2.10.0 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from tensorflow-gpu) (2.10.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from tensorboard~=2.4->tensorflow-gpu) (1.24.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from tensorboard~=2.4->tensorflow-gpu) (1.7.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from tensorboard~=2.4->tensorflow-gpu) (3.3.3)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from tensorboard~=2.4->tensorflow-gpu) (1.0.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from tensorboard~=2.4->tensorflow-gpu) (49.6.0.post20200925)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from tensorboard~=2.4->tensorflow-gpu) (0.4.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from tensorboard~=2.4->tensorflow-gpu) (2.25.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow-gpu) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow-gpu) (4.2.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow-gpu) (4.6)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow-gpu) (2.0.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow-gpu) (1.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow-gpu) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow-gpu) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow-gpu) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow-gpu) (1.26.2)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow-gpu) (0.4.8)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow-gpu) (3.4.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\programdata\\anaconda3\\envs\\lab_1\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow-gpu) (3.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy\n",
    "!pip install  gym\n",
    "!pip install keras\n",
    "!pip install matplotlib\n",
    "!pip install opencv-python\n",
    "!pip install tensorflow-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "from keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import error\n",
    "\n",
    "try:\n",
    "    import atari_py\n",
    "except ImportError as e:\n",
    "    raise error.DependencyNotInstalled(\n",
    "        \"{}. (HINT: you can install Atari dependencies by running \"\n",
    "        \"'pip install gym[atari]'.)\".format(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    outputs = 2\n",
    "    # keras example for breakout\n",
    "    inputs = layers.Input(shape=(105, 80, 4), dtype=np.float32)\n",
    "    layer1 = layers.Conv2D(32, 8, strides=4, activation=\"relu\")(inputs)\n",
    "    layer2 = layers.Conv2D(64, 4, strides=2, activation=\"relu\")(layer1)\n",
    "    layer3 = layers.Conv2D(64, 3, strides=1, activation=\"relu\")(layer2)\n",
    "    layer4 = layers.Flatten()(layer3)\n",
    "    layer5 = layers.Dense(512, activation=\"relu\")(layer4)\n",
    "    action = layers.Dense(outputs, activation=\"linear\")(layer5)\n",
    "    model = keras.Model(inputs=inputs, outputs=action)\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=0.00025, clipnorm=1.0)  # faster than rmsprop\n",
    "    model.compile(optimizer, loss=keras.losses.Huber())  # Huber for stability\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 105, 80, 4)]      0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 25, 19, 32)        8224      \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 11, 8, 64)         32832     \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 9, 6, 64)          36928     \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 3456)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 512)               1769984   \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2)                 1026      \n",
      "=================================================================\n",
      "Total params: 1,848,994\n",
      "Trainable params: 1,848,994\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "second_model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb_to_greyscale(observation):\n",
    "    observation = observation[:, :, 0] + observation[:, :, 1] + observation[:, :, 2]\n",
    "    return np.where(observation > 0, 255, 0)[::2, ::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable declaration\n",
    "state_history = []\n",
    "action_history = []\n",
    "reward_history = []\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay( # initial learning rate tried from 0.0001 to 0.1\n",
    "    initial_learning_rate=0.001,\n",
    "    decay_steps=100000,\n",
    "    decay_rate=0.25\n",
    ")\n",
    "optimizer = keras.optimizers.Adam(learning_rate=lr_schedule, clipnorm=1.0)  # faster than rmsprop\n",
    "loss_function = keras.losses.Huber()  # used for stability, tried MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "epsilon = 1.0\n",
    "minimum_epsilon = 0.05  # values tried from 0.01 to 0.2\n",
    "gamma = 0.5  # values tried from 0.5 to 0.99\n",
    "max_gamma = 0.99\n",
    "previous_lives = 5\n",
    "max_memory_length = 10000  # good for local runs, 100000 for colaboratory\n",
    "improvement_check = 100\n",
    "iterations = 0\n",
    "games_played = 0\n",
    "update_second_model = 5000\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "978\n",
      "1833\n",
      "2502\n",
      "3281\n",
      "4060\n",
      "4839\n",
      "5618\n",
      "6097\n",
      "6764\n",
      "Reward: 1.0, games played: 10, iterations made: 7543\n",
      "7543\n",
      "8208\n",
      "9071\n",
      "9850\n",
      "10903\n",
      "11568\n",
      "12047\n",
      "13140\n",
      "13991\n",
      "14470\n",
      "Reward: -1.0, games played: 20, iterations made: 14949\n",
      "14949\n",
      "15728\n",
      "16507\n",
      "16986\n",
      "17465\n",
      "17944\n",
      "18423\n",
      "18902\n",
      "19381\n",
      "19974\n",
      "Reward: 1.0, games played: 30, iterations made: 20753\n",
      "20753\n",
      "21232\n",
      "22215\n",
      "22694\n",
      "23473\n",
      "24252\n",
      "24845\n",
      "25324\n",
      "26103\n",
      "26582\n",
      "Reward: -1.0, games played: 40, iterations made: 27061\n",
      "27061\n",
      "27726\n",
      "28205\n",
      "28798\n",
      "29391\n",
      "30284\n",
      "30763\n",
      "31937\n",
      "33123\n",
      "33788\n",
      "Reward: 2.0, games played: 50, iterations made: 34689\n",
      "34689\n",
      "35354\n",
      "36247\n",
      "36726\n",
      "37319\n",
      "37912\n",
      "38577\n",
      "39170\n",
      "39949\n",
      "40850\n",
      "Reward: 0.0, games played: 60, iterations made: 41443\n",
      "41443\n",
      "41922\n",
      "42701\n",
      "43180\n",
      "43659\n",
      "44324\n",
      "44999\n",
      "45676\n",
      "46155\n",
      "47028\n",
      "Reward: 1.0, games played: 70, iterations made: 47817\n",
      "47817\n",
      "48482\n",
      "49261\n",
      "50040\n",
      "50819\n",
      "51799\n",
      "52692\n",
      "53471\n",
      "54064\n",
      "54937\n",
      "Reward: -1.0, games played: 80, iterations made: 55416\n",
      "55416\n",
      "55895\n",
      "56560\n",
      "57339\n",
      "57818\n",
      "58679\n",
      "59386\n",
      "59865\n",
      "61038\n",
      "61817\n",
      "Reward: -1.0, games played: 90, iterations made: 62296\n",
      "62296\n",
      "63637\n",
      "64500\n",
      "65369\n",
      "65962\n",
      "66741\n",
      "67220\n",
      "67699\n",
      "68560\n",
      "69039\n",
      "Reward: 0.0, games played: 100, iterations made: 69632\n",
      "69632\n",
      "70497\n",
      "71526\n",
      "72575\n",
      "73168\n",
      "73833\n",
      "74612\n",
      "75577\n",
      "76356\n",
      "77177\n",
      "Reward: -1.0, games played: 110, iterations made: 77656\n",
      "77656\n",
      "78549\n",
      "79028\n",
      "79621\n",
      "80100\n",
      "80579\n",
      "81434\n",
      "82109\n",
      "82702\n",
      "83563\n",
      "Reward: 1.0, games played: 120, iterations made: 84342\n",
      "84342\n",
      "84821\n",
      "85600\n",
      "86689\n",
      "87168\n",
      "87647\n",
      "88612\n",
      "89091\n",
      "89684\n",
      "90163\n",
      "Reward: -1.0, games played: 130, iterations made: 90642\n",
      "90642\n",
      "91311\n",
      "91790\n",
      "92269\n",
      "92862\n",
      "94299\n",
      "94778\n",
      "95257\n",
      "96036\n",
      "97009\n",
      "Reward: 2.0, games played: 140, iterations made: 97906\n",
      "97906\n",
      "98385\n",
      "98864\n",
      "99343\n",
      "100010\n",
      "100489\n",
      "101268\n",
      "101747\n",
      "102226\n",
      "102819\n",
      "Reward: -1.0, games played: 150, iterations made: 103298\n",
      "103298\n",
      "103891\n",
      "104566\n",
      "105231\n",
      "105824\n",
      "106303\n",
      "107082\n",
      "107861\n",
      "108640\n",
      "109419\n",
      "Reward: 1.0, games played: 160, iterations made: 110198\n",
      "110198\n",
      "110977\n",
      "111756\n",
      "112535\n",
      "113128\n",
      "113907\n",
      "114768\n",
      "116023\n",
      "116692\n",
      "117285\n",
      "Reward: -1.0, games played: 170, iterations made: 117764\n",
      "117764\n",
      "118479\n",
      "119072\n",
      "119665\n",
      "120330\n",
      "120923\n",
      "121516\n",
      "121995\n",
      "122664\n",
      "123707\n",
      "Reward: 2.0, games played: 180, iterations made: 124681\n",
      "124681\n",
      "125160\n",
      "126173\n",
      "126766\n",
      "127748\n",
      "128581\n",
      "129174\n",
      "129767\n",
      "130246\n",
      "130915\n",
      "Reward: 1.0, games played: 190, iterations made: 131694\n",
      "131694\n",
      "132473\n",
      "133760\n",
      "134239\n",
      "135290\n",
      "136335\n",
      "137432\n",
      "138325\n",
      "138804\n",
      "139583\n",
      "Reward: 1.0, games played: 200, iterations made: 140362\n",
      "140362\n",
      "141141\n",
      "141920\n",
      "142781\n",
      "143927\n",
      "144520\n",
      "144999\n",
      "145478\n",
      "145957\n",
      "146550\n",
      "Reward: -1.0, games played: 210, iterations made: 147029\n",
      "147029\n",
      "147622\n",
      "148218\n",
      "148811\n",
      "149290\n",
      "149883\n",
      "150476\n",
      "150955\n",
      "151548\n",
      "152027\n",
      "Reward: -1.0, games played: 220, iterations made: 152506\n",
      "152506\n",
      "152985\n",
      "153814\n",
      "154707\n",
      "155486\n",
      "156455\n",
      "157128\n",
      "157907\n",
      "158386\n",
      "158979\n",
      "Reward: 1.0, games played: 230, iterations made: 159838\n",
      "159838\n",
      "160317\n",
      "160910\n",
      "161887\n",
      "162758\n",
      "163659\n",
      "164336\n",
      "164815\n",
      "165294\n",
      "166073\n",
      "Reward: 1.0, games played: 240, iterations made: 166852\n",
      "166852\n",
      "167631\n",
      "168410\n",
      "169303\n",
      "170082\n",
      "170861\n",
      "171640\n",
      "172419\n",
      "173198\n",
      "173977\n",
      "Reward: 1.0, games played: 250, iterations made: 174756\n",
      "174756\n",
      "175535\n",
      "176314\n",
      "177093\n",
      "177872\n",
      "178651\n",
      "179430\n",
      "180209\n",
      "180988\n",
      "181955\n",
      "Reward: -1.0, games played: 260, iterations made: 182434\n",
      "182434\n",
      "183213\n",
      "183992\n",
      "184773\n",
      "185366\n",
      "185845\n",
      "186438\n",
      "187031\n",
      "188320\n",
      "188799\n",
      "Reward: -1.0, games played: 270, iterations made: 189278\n",
      "189278\n",
      "190057\n",
      "190722\n",
      "191315\n",
      "191908\n",
      "192501\n",
      "193741\n",
      "194634\n",
      "195527\n",
      "196192\n",
      "Reward: 0.0, games played: 280, iterations made: 196785\n",
      "196785\n",
      "197564\n",
      "198343\n",
      "199763\n",
      "200428\n",
      "201021\n",
      "201614\n",
      "202093\n",
      "203649\n",
      "204242\n",
      "Reward: 0.0, games played: 290, iterations made: 204835\n",
      "204835\n",
      "205314\n",
      "205907\n",
      "206500\n",
      "207333\n",
      "207926\n",
      "208897\n",
      "209490\n",
      "209969\n",
      "210866\n",
      "Reward: 1.0, games played: 300, iterations made: 211645\n",
      "211645\n",
      "212424\n",
      "213203\n",
      "213982\n",
      "214761\n",
      "215654\n",
      "216433\n",
      "217398\n",
      "217991\n",
      "219518\n",
      "Reward: 1.0, games played: 310, iterations made: 220297\n",
      "220297\n",
      "220776\n",
      "221555\n",
      "222334\n",
      "223113\n",
      "223892\n",
      "224671\n",
      "225450\n",
      "226343\n",
      "227132\n",
      "Reward: 0.0, games played: 320, iterations made: 227725\n",
      "227725\n",
      "228318\n",
      "229151\n",
      "229744\n",
      "230223\n",
      "231002\n",
      "231481\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-31-b172b9703dc3>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     14\u001B[0m             \u001B[0mfour_states\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0marray\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mstate_history\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m-\u001B[0m\u001B[1;36m4\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# takes last 4 known states\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     15\u001B[0m             \u001B[0mfour_states\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mfour_states\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mreshape\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m105\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m80\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m4\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# reshapes them into input shape\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 16\u001B[1;33m             \u001B[0mpredictions\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpredict\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0marray\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mfour_states\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mverbose\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# gets reward predictions for both actions\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     17\u001B[0m             \u001B[0maction\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;36m2\u001B[0m \u001B[1;32mif\u001B[0m \u001B[0mpredictions\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m>\u001B[0m \u001B[0mpredictions\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m \u001B[1;33m,\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;32melse\u001B[0m \u001B[1;36m3\u001B[0m  \u001B[1;31m# choses the actions with the greatest predicted reward\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     18\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\ProgramData\\Anaconda3\\envs\\Lab_1\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001B[0m in \u001B[0;36mpredict\u001B[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[0;32m   1623\u001B[0m       \u001B[0mcallbacks\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mon_predict_begin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1624\u001B[0m       \u001B[0mbatch_outputs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1625\u001B[1;33m       \u001B[1;32mfor\u001B[0m \u001B[0m_\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0miterator\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mdata_handler\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0menumerate_epochs\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m  \u001B[1;31m# Single epoch.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1626\u001B[0m         \u001B[1;32mwith\u001B[0m \u001B[0mdata_handler\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcatch_stop_iteration\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1627\u001B[0m           \u001B[1;32mfor\u001B[0m \u001B[0mstep\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mdata_handler\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msteps\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\ProgramData\\Anaconda3\\envs\\Lab_1\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001B[0m in \u001B[0;36menumerate_epochs\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1131\u001B[0m     \u001B[1;34m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1132\u001B[0m     \u001B[1;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_truncate_execution_to_epoch\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1133\u001B[1;33m       \u001B[0mdata_iterator\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0miter\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_dataset\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1134\u001B[0m       \u001B[1;32mfor\u001B[0m \u001B[0mepoch\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_initial_epoch\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_epochs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1135\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_insufficient_data\u001B[0m\u001B[1;33m:\u001B[0m  \u001B[1;31m# Set by `catch_stop_iteration`.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\ProgramData\\Anaconda3\\envs\\Lab_1\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001B[0m in \u001B[0;36m__iter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    420\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mcontext\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexecuting_eagerly\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mor\u001B[0m \u001B[0mops\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0minside_function\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    421\u001B[0m       \u001B[1;32mwith\u001B[0m \u001B[0mops\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcolocate_with\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_variant_tensor\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 422\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0miterator_ops\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mOwnedIterator\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    423\u001B[0m     \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    424\u001B[0m       raise RuntimeError(\"__iter__() is only supported inside of tf.function \"\n",
      "\u001B[1;32mC:\\ProgramData\\Anaconda3\\envs\\Lab_1\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, dataset, components, element_spec)\u001B[0m\n\u001B[0;32m    680\u001B[0m       \u001B[1;32mif\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mcomponents\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m \u001B[1;32mor\u001B[0m \u001B[0melement_spec\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    681\u001B[0m         \u001B[1;32mraise\u001B[0m \u001B[0mValueError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0merror_message\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 682\u001B[1;33m       \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_create_iterator\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    683\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    684\u001B[0m   \u001B[1;32mdef\u001B[0m \u001B[0m_create_iterator\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdataset\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\ProgramData\\Anaconda3\\envs\\Lab_1\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\u001B[0m in \u001B[0;36m_create_iterator\u001B[1;34m(self, dataset)\u001B[0m\n\u001B[0;32m    703\u001B[0m               \u001B[0moutput_types\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_flat_output_types\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    704\u001B[0m               output_shapes=self._flat_output_shapes))\n\u001B[1;32m--> 705\u001B[1;33m       \u001B[0mgen_dataset_ops\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmake_iterator\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mds_variant\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_iterator_resource\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    706\u001B[0m       \u001B[1;31m# Delete the resource when this object is deleted\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    707\u001B[0m       self._resource_deleter = IteratorResourceDeleter(\n",
      "\u001B[1;32mC:\\ProgramData\\Anaconda3\\envs\\Lab_1\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py\u001B[0m in \u001B[0;36mmake_iterator\u001B[1;34m(dataset, iterator, name)\u001B[0m\n\u001B[0;32m   2969\u001B[0m     \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2970\u001B[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001B[1;32m-> 2971\u001B[1;33m         _ctx, \"MakeIterator\", name, dataset, iterator)\n\u001B[0m\u001B[0;32m   2972\u001B[0m       \u001B[1;32mreturn\u001B[0m \u001B[0m_result\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2973\u001B[0m     \u001B[1;32mexcept\u001B[0m \u001B[0m_core\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_NotOkStatusException\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "env = gym.make(\"BreakoutNoFrameskip-v4\")  # tried v0 and deterministic\n",
    "while True:  # training continues forever\n",
    "    print(iterations)\n",
    "    observation = env.reset()  # reseting env\n",
    "    state = rgb_to_greyscale(observation)  # converts observation (210, 160, 3), to greydownscaled state, (105, 80)\n",
    "\n",
    "    # when predictiong only LEFT or RIGHT, we have to manually FIRE\n",
    "    env.step(1)  # FIRE triggers ball initialization\n",
    "\n",
    "    episode_reward = 0  # initializes reward for the new episode which follows\n",
    "\n",
    "    while True:  # agent still has lives\n",
    "        # env.render()\n",
    "\n",
    "        iterations += 1  # increasing total iterations of game\n",
    "        if epsilon > np.random.rand(1)[0] or len(state_history) < 4:  # a leap of fate, exploration\n",
    "            # case of model output being only LEFT / RIGHT\n",
    "            action = np.random.randint(2, 4)  # random left or right | 3 is left, 2 is right\n",
    "\n",
    "            # case of model output being all 4 actions: NOOP, FIRE, LEFT, RIGHT\n",
    "            # action = np.random.choice(4)\n",
    "        else:  # agent must predict action, exploitation\n",
    "            four_states = np.array(state_history[-4:])  # takes last 4 known states\n",
    "            four_states = four_states.reshape((105, 80, 4))  # reshapes them into input shape\n",
    "\n",
    "            predictions = model.predict(np.array([four_states]), verbose=0)  # gets reward predictions for both actions\n",
    "            # case of model output being only LEFT / RIGHT\n",
    "            action = 2 + np.argmax(predictions[0])  # choses the actions with the greatest predicted reward\n",
    "            # case of model output being all 4 actions: NOOP, FIRE, LEFT, RIGHT\n",
    "            # action = np.argmax(predictions[0])\n",
    "\n",
    "        if epsilon > minimum_epsilon:  # decay over time is applied to epsilon until it reaches critical value\n",
    "            epsilon -= epsilon / 1000000  # critical value is reached in 2995732 steps\n",
    "\n",
    "        # we should be more interested about total reward later in training, at the beggining we should be greedy for reward\n",
    "        if gamma < max_gamma:\n",
    "            gamma += gamma / 1000000\n",
    "\n",
    "        observation, reward, done, info = env.step(action)  # action is played, returns new observation, possible reward, done flag and lives remained\n",
    "        next_state = rgb_to_greyscale(observation)  # converts observation (210, 160, 3), to greydownscaled state, (105, 80)\n",
    "\n",
    "        if info[\"ale.lives\"] != previous_lives:  # if number of lives decreased during this frame\n",
    "            env.step(1)  # FIRE summons ball\n",
    "            previous_lives = info[\"ale.lives\"]  # updates previous_lives with current lives\n",
    "            # here we tried different negative rewards, including no negative reward for life lost and including no negative reward for DONE\n",
    "            # reward -= 1  # updates reward with negative value because a life was lost\n",
    "            if done:  # if game is finished, agent lost\n",
    "                reward -= 1\n",
    "\n",
    "        # we also tried this, the purpose being to encourage the agent to try to make the ball reach above bricks and finish the game faster, but it never reached that state\n",
    "        # if reward == 0:  # if no reward is received\n",
    "        #     reward -= 0.1  # reward receives small negative value, should encourage the agent to finish the game faster\n",
    "\n",
    "        # saving values\n",
    "        state_history.append(state)\n",
    "        action_history.append(action)\n",
    "        reward_history.append(reward)\n",
    "        state = next_state  # replaces old state with new one\n",
    "\n",
    "        episode_reward += reward  # increases reward for this episode, for checking out improvements for games\n",
    "\n",
    "        # Start Back Prop\n",
    "\n",
    "        if iterations % 4 == 0 and len(action_history) > batch_size:  # doing backprop once every 4, 16 or 32 steps\n",
    "            # prepare values to be used for backpropagation\n",
    "            indices = np.random.choice(range(4, len(action_history) - 1),\n",
    "                                       size=batch_size)  # get only indices that have at least 4 previous states, and 1 next state\n",
    "\n",
    "            state_sample = np.array([state_history[i - 4:i] for i in\n",
    "                                     indices])  # takes groups of 4 images of game board, previous and except current index\n",
    "            state_sample = state_sample.reshape((batch_size, 105, 80, 4))  # reshapes group from (32, 4, 105, 80) to (32, 105, 80, 4)\n",
    "\n",
    "            next_state_sample = np.array([state_history[i - 3: i + 1] for i in\n",
    "                                          indices])  # takes gropus of 4 images of game board, previous and including current index\n",
    "            next_state_sample = next_state_sample.reshape((batch_size, 105, 80, 4))\n",
    "            reward_sample = np.array([reward_history[i] for i in indices])  # has shape (32,)\n",
    "            action_sample = [action_history[i] - 2 for i in indices]\n",
    "            # has len 32; 2 is decreased from each action to transform it into 0 or 1, to minimize one_hot_vectors size\n",
    "            # when using all 4 actions, -2 is not decreased\n",
    "            future_rewards = np.amax(second_model.predict(next_state_sample, verbose=0), axis=1)\n",
    "            # gets maximum prediction using second model of future rewards for each next state sample\n",
    "            updated_q_values = reward_sample + gamma * future_rewards  # for current state, adds reward obtained to next state predicted max reward\n",
    "            masks = to_categorical(action_sample,\n",
    "                                   num_classes=2)  # one hot masks are created for actions, to apply backprop only for chosen actions\n",
    "\n",
    "            # The code for when we tried to do backprop by using model.fit\n",
    "            #             q_values = model.predict(state_sample, verbose=0)  # here we predict the values\n",
    "            #             q_action = np.sum(q_values * masks, axis=1)  # here we get the reward predicted for chosen action\n",
    "\n",
    "            #             true_labels = second_model.predict(next_state_sample, verbose=0)  # here we get again the predictions\n",
    "            #             j = 0\n",
    "            #             for i in true_labels:\n",
    "            #                 true_labels[np.argmax(i)] = updated_q_values[j]  # here we modify predictions for chosen actions with updated_q_values = reward  + gamma*  next_state prediction\n",
    "            #                 j += 1\n",
    "            #             state_sample = tf.convert_to_tensor(state_sample)\n",
    "            #             true_labels = tf.convert_to_tensor(true_labels)\n",
    "            #             model.fit(\n",
    "            #                 state_sample,\n",
    "            #                 true_labels,\n",
    "            #                 epochs=1,\n",
    "            #                 verbose=0,\n",
    "            #                 batch_size=32\n",
    "            #             )\n",
    "\n",
    "            # faster (and better) than model.fit\n",
    "            with tf.GradientTape() as tape:  # Copied example from keras q-learning. Applies backpropagation to model\n",
    "                # here we needed to use tensors, otherwise error\n",
    "                # feed forward\n",
    "                q_values = model(\n",
    "                    state_sample)  # same as `q_values = model.predict(state_sample, verbose=0)`, but returns tensor\n",
    "                q_action = tf.reduce_sum(tf.multiply(q_values, masks),\n",
    "                                         axis=1)  # same as `q_action = np.sum(q_values * masks, axis=1)`, but returns tensor\n",
    "\n",
    "                # backpropagation\n",
    "\n",
    "                loss = loss_function(updated_q_values, q_action)\n",
    "                # calculates the loss between updated_q_values, which are correct labels expected, and q_action is the output obtained\n",
    "                grads = tape.gradient(loss, model.trainable_variables)  # applies gradient to weights\n",
    "                optimizer.apply_gradients(zip(grads, model.trainable_variables))  # uses optimizer to update weigths\n",
    "\n",
    "        # End Back Prop\n",
    "\n",
    "        if iterations % update_second_model == 0:  # once every 5000/other numbers iterations\n",
    "            second_model.set_weights(model.get_weights())  # updates second model\n",
    "\n",
    "        if len(action_history) > max_memory_length:  # if max memory was reached\n",
    "            del state_history[:1]  # deletes the oldest state from each list, we also tried to delete 100, 1000, 5000, but slower convergence\n",
    "            del action_history[:1]\n",
    "            del reward_history[:1]\n",
    "\n",
    "        if done:  # end game flag\n",
    "            games_played += 1  # increasing played games\n",
    "            if games_played % 10 == 0:  # once every 100 games, save the model\n",
    "                model.save(\"a1.h5\")\n",
    "                print(f\"Reward: {episode_reward}, games played: {games_played}, iterations made: {iterations}\")\n",
    "            break  # exits current game\n",
    "\n",
    "    if iterations % 10000 == 0:\n",
    "        print(f\"Reward: {episode_reward}, games played: {games_played}, iterations made: {iterations}\")\n",
    "        print(games_played)\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.096"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def decayed_learning_rate(step):\n",
    "  return 0.1 * (0.96 ** (step / 100000))\n",
    "\n",
    "bb = 0.1\n",
    "aa = 0\n",
    "while True:\n",
    "    aa += 1\n",
    "    if aa > 100000:\n",
    "        break\n",
    "    bb = decayed_learning_rate(aa)\n",
    "    print(bb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}